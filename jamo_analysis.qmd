---
title: "Jamo Analysis"
author: "Ryan Jiang"
format: pdf
editor: visual
---



```{r}
library(tidyverse)
library(ggplot2)
library(stringr)
library(dplyr)
library(purrr)
```

```{r}
unique <- readRDS("~/YKim-Linguistics/ykim_korean-analysis/unique_database.rds")
```

```{r}
# Creates a summation of the appearance of all the rules across the dataset - Not moving in this direction anymore, check google doc
#rules_summary <- only_U_1100 |>
#  mutate(rule_sums = map(Jamo_Rule_Matrix, ~ colSums(.x[,-1], na.rm = TRUE))) |>
#  pull(rule_sums) |>
#  bind_rows() |>
#  summarize(across(everything(), sum))

```

```{r}
# NOT DOING THIS ANYMORE

#U_1100_rules_counts <- rules_summary |>
#  pivot_longer(everything(), names_to = "rule", values_to = "count")

#ggplot(U_1100_rules_counts, aes(x = reorder(rule, count), y = count)) +
#  geom_col() +
#  coord_flip() + 
#  labs(
#    x = "Rule",
#    y = "Counts",
#    title = "Rules in order of frequency of words/phrases containing U+1100"
#  ) +
#  theme_minimal()

```


```{r}

# creates a list of all of the existing Jamo Unicodes
unicode_list <- unique |>
  mutate(uc = str_extract_all(Jamo_Cleaned_Unicode, "U\\+[0-9A-F]{4}")) |>
  unnest(uc) |>
  distinct(uc) |>
  pull(uc)

# preallocation of list size
unicode_proportions <- vector("list", length(unicode_list))

# iterating through and detecting if the unicode exists within the list of unicodes of a given word
for (i in seq_along(unicode_list)) {
  u <- unicode_list[i]
  unicode_proportions[[i]] <- data.frame(
    unicode = u,
    prop_included = mean(str_detect(unique[["Jamo_Cleaned_Unicode"]], fixed(u))),
    stringsAsFactors = FALSE
  )
}

unicode_df <- bind_rows(unicode_proportions)

# plotting data
unicode_proportions <- ggplot(unicode_df, aes(x = reorder(unicode, prop_included), y = prop_included)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(
    y = "Proportion Included",
    x = "Jamo Unicode",
    title = "Proportion of Each Jamo Unicode in the Dataset"
  )

ggsave("unicode_proportions.pdf", plot = unicode_proportions, width = 8, height = 6)

```

```{r}
# creates a list of all of the existing IPA Unicodes
ipa_unicode_list <- unique |>
  mutate(uc = str_extract_all(IPA_Syllable_Unicode, "U\\+[0-9A-F]{4}")) |>
  unnest(uc) |>
  distinct(uc) |>
  pull(uc)

# preallocation of list size
ipa_unicode_proportions <- vector("list", length(ipa_unicode_list))

# iterating through and detecting if the IPA unicode exists within the list of unicodes of a given word
for (i in seq_along(ipa_unicode_list)) {
  u <- ipa_unicode_list[i]
  ipa_unicode_proportions[[i]] <- data.frame(
    unicode = u,
    prop_included = mean(str_detect(unique[["IPA_Syllable_Unicode"]], fixed(u))),
    stringsAsFactors = FALSE
  )
}

ipa_unicode_df <- bind_rows(ipa_unicode_proportions)

# plotting data
ipa_unicode_proportions <- ggplot(ipa_unicode_df, aes(x = reorder(unicode, prop_included), y = prop_included)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(
    y = "Proportion Included",
    x = "IPA Unicode",
    title = "Proportion of Each IPA Unicode in the Dataset"
  )

ggsave("ipa_unicode_proportions.pdf", plot = ipa_unicode_proportions, width = 8, height = 6)

```




```{r}
# creating list of names based on column names of the matrices
rules <- colnames(unique$Jamo_Rule_Matrix[[1]])[-1]

# preallocation of the list of proportions
rule_proportions <- vector("list", length(rules))


# iterating through each rule and detecting if the matrices contain the rule
for (i in seq_along(rules)) {
  r <- rules[i]
  
  contains_rule <- map_lgl(unique$Jamo_Rule_Matrix, ~ any(.x[, r] == 1))
  
  proportion_removed <- mean(contains_rule)
  
  rule_proportions[[i]] <- tibble(
    rule = r,
    prop_removed = proportion_removed
  )
}

rule_proportion_df <- bind_rows(rule_proportions)


# plotting df
rule_removed_proportions <- ggplot(rule_proportion_df, aes(x = reorder(rule, prop_removed), y = prop_removed)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(
    y = "Proportion Removed",
    x = "Rules",
    title = "Proportion of Words Removed Upon Removing Certain Rules"
  )

ggsave("rule_removed_proportions.pdf", plot = rule_removed_proportions, width = 8, height = 6)

```

Working with frequency values rather than the unique word set to account for relevance and usage of certain words for more realistic distributions.

```{r}
# dataset with only values with frequency and proportions
clean_unique <- unique |>
  filter(!is.na(total_instances), !is.na(freq))

# dataset with only values without frequency and proportions
na_unique <- unique |>
  filter(is.na(total_instances), is.na(freq))
```


```{r}
# find frequency of each unicode
unicode_freq <- clean_unique |>
  mutate(uni = str_extract_all(Jamo_Cleaned_Unicode, "U\\+[0-9A-F]{4}")) |>
  unnest(uni) |>
  group_by(uni) |>
  summarize(total_freq = sum(freq), .groups = "drop")

# calculate the proportion
unicode_freq <- unicode_freq |>
  mutate(prop = total_freq / sum(total_freq))

unicode_freq |>
  summarize(sum = sum(prop))

# contains 1053565	instances of all words 
clean_unique |>
  summarize(sum = sum(freq))

# make plot based on proportions
clean_uni_proportions <- unicode_freq |>
  ggplot(aes(x = reorder(uni, prop), y = prop)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(
    x = "Unicode",
    y = "Unicode Proportion",
    title = "Unicode Frequency (Weighted by Word Frequencies)"
  )

ggsave("clean_uni_proportions.pdf", plot = clean_uni_proportions, width = 8, height = 6)


clean_uni_freq <- unicode_freq |>
  ggplot(aes(x = reorder(uni, total_freq), y = total_freq)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(
    x = "Unicode",
    y = "Unicode Frequencies",
    title = "Unicode Frequency (Weighted by Word Frequencies)"
  )

ggsave("clean_uni_frequencies.pdf", plot = clean_uni_freq, width = 8, height = 6)
```


```{r}
# pull rules from Matrix column names
clean_uni_rule_proportions <- vector("list", length(rules))

# iterate through each rule and check in the word list with only frequency values to find proportion of words removed based on rule
for (i in seq_along(rules)) {
  r <- rules[i]
  
  contains_rule <- map_lgl(clean_unique$Jamo_Rule_Matrix, ~ any(.x[, r] == 1))
  
  total_freq <- sum(clean_unique$freq, na.rm = TRUE)
  weighted_prop <- sum(clean_unique$freq[contains_rule], na.rm = TRUE) / total_freq
  
  clean_uni_rule_proportions[[i]] <- tibble(
    rule = r,
    prop_weighted = weighted_prop
  )
}

clean_uni_rule_proportion_df <- bind_rows(clean_uni_rule_proportions)

clean_uni_rule_prop <- clean_uni_rule_proportion_df |>
  ggplot(aes(x = reorder(rule, prop_weighted), y = prop_weighted)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(
    x = "Rule",
    y = "Rule Proportion",
    title = "Rule Proportions Removed (Weighted by Word Frequencies)"
  )

ggsave("clean_uni_rule_prop.pdf", plot = clean_uni_rule_prop, width = 8, height = 6)
```


```{r}
# GRAMMATICAL PARTS

# pulling all instances of unique grammatical parts to iterate over later
grammatical_parts <- clean_unique |>
  separate_rows(품사, sep = "\\s*,\\s*") |>
  pull(품사) |>
  unique()

# split grammatical parts into multiple rows/observations with a single grammatical part
clean_unique_parts <- clean_unique |>
  separate_rows(품사, sep = "\\s*,\\s*")

# calculating frequency by grammatical part to account for a word's relevance in Hangul
parts_totals <- clean_unique_parts |>
  group_by(품사) |>
  summarize(total_freq = sum(freq), .groups = "drop")

# function to calculate the proportion of each grammatical part is removed when a specific rule is removed
rule_parts_prop <- map_dfr(rules, function(r) {

  affected <- map_lgl(
    clean_unique_parts$Jamo_Rule_Matrix,
    ~ any(.x[, r] == 1)
  )

  clean_unique_parts[affected, ] |>
    group_by(품사) |>
    summarize(removed_freq = sum(freq), .groups = "drop") |>
    left_join(parts_totals, by = "품사") |>
    mutate(
      rule = r,
      prop_removed = removed_freq / total_freq
    ) |>
    select(품사, rule, prop_removed)
})

# iterate through each grammatical part and plot its respective proportions removed by each rule
walk(grammatical_parts, function(part) {

  p <- ggplot(
    rule_parts_prop |> filter(품사 == part),
    aes(x = reorder(rule, prop_removed), y = prop_removed)
  ) +
    geom_col() +
    coord_flip() +
    theme_minimal() +
    labs(
      x = "Rule Removed",
      y = "Proportion of Grammatical Part Removed",
      title = paste("Effect of Rule Removal —", part)
    )

  print(p)
})
```


```{r}
# Grammatical Parts by Jamo Unicode

# splitting up cells with multiple grammatical parts and Jamo unicode into individual rows
clean_unique_long <- clean_unique |>
  separate_rows(품사, sep = "\\s*,\\s*") |>
  separate_rows(Jamo_Cleaned_Unicode, sep = "\\s+")

# calculating proportion of a grammatical part is removed when a specific rule is removed
jamo_parts_prop <- clean_unique_long |>
  group_by(품사, Jamo_Cleaned_Unicode) |>
  summarize(
    removed_freq = sum(freq),
    .groups = "drop"
  ) |>
  left_join(parts_totals, by = "품사") |>
  mutate(
    prop_removed = removed_freq / total_freq
  ) |>
  rename(jamo_unicode = Jamo_Cleaned_Unicode)

# iterating through each unique grammatical part and plotting each distribution plot
walk(grammatical_parts, function(parts) {

  df <- jamo_parts_prop |> filter(품사 == parts)
  if (nrow(df) == 0) return(NULL)

  p <- ggplot(
    df,
    aes(x = reorder(jamo_unicode, prop_removed),
        y = prop_removed)
  ) +
    geom_col() +
    coord_flip() +
    theme_minimal() +
    labs(
      x = "Jamo Unicode Removed",
      y = "Proportion of Grammatical Part Removed",
      title = paste("Effect of Jamo Unicode Removal —", parts)
    )

  print(p)
})


```

```{r}
# WORD ORIGINS

# pulling unique word origins
word_origins <- clean_unique |>
  separate_rows(고유어_여부, sep = "\\s*,\\s*") |>
  pull(고유어_여부) |>
  unique()

# split word origins into multiple rows
clean_unique_origins <- clean_unique |>
  separate_rows(고유어_여부, sep = "\\s*,\\s*")

# calculating total frequencies for each word origin category
origin_totals <- clean_unique_origins |>
  group_by(고유어_여부) |>
  summarize(total_freq = sum(freq), .groups = "drop")

# calculating proportion of each word origin category removed when a specific rule is removed
rule_origin_prop <- map_dfr(rules, function(r) {

  affected <- map_lgl(
    clean_unique_origins$Jamo_Rule_Matrix,
    ~ any(.x[, r] == 1)
  )

  clean_unique_origins[affected, ] |>
    group_by(고유어_여부) |>
    summarize(removed_freq = sum(freq), .groups = "drop") |>
    left_join(origin_totals, by = "고유어_여부") |>
    mutate(
      rule = r,
      prop_removed = removed_freq / total_freq
    ) |>
    select(고유어_여부, rule, prop_removed)
})

# iterating through each word origin category and creating a plot displaying the proportion of a word origin category removed when a rule is removed
walk(word_origins, function(origin) {

  p <- ggplot(
    rule_origin_prop |> filter(고유어_여부 == origin),
    aes(x = reorder(rule, prop_removed), y = prop_removed)
  ) +
    geom_col() +
    coord_flip() +
    theme_minimal() +
    labs(
      x = "Rule Removed",
      y = "Proportion of Word Origin Removed",
      title = paste("Effect of Rule Removal —", origin)
    )

  print(p)
})

```

```{r}
# Word Origins by Jamo Unicode

# creating a dataframe with each origin category and unicode split into separate rows
clean_unique_origins_unicode <- clean_unique |>
  separate_rows(고유어_여부, sep = "\\s*,\\s*") |>
  separate_rows(Jamo_Cleaned_Unicode, sep = "\\s+")

# calculating proportion of each word origin category removed when a Jamo unicode is removed
jamo_origins_prop <- clean_unique_origins_unicode |>
  group_by(고유어_여부, Jamo_Cleaned_Unicode) |>
  summarize(
    removed_freq = sum(freq),
    .groups = "drop"
  ) |>
  left_join(origin_totals, by = "고유어_여부") |>
  mutate(
    prop_removed = removed_freq / total_freq
  ) |>
  rename(jamo_unicode = Jamo_Cleaned_Unicode)

# iterating through each word origin category and creating a plot displaying the proportion of a word origin category removed when a unicode is removed
walk(word_origins, function(origin) {

  df <- jamo_origins_prop |> filter(고유어_여부 == origin)
  if (nrow(df) == 0) return(NULL)

  p <- ggplot(
    df,
    aes(x = reorder(jamo_unicode, prop_removed),
        y = prop_removed)
  ) +
    geom_col() +
    coord_flip() +
    theme_minimal() +
    labs(
      x = "Jamo Unicode Removed",
      y = "Proportion of Word Origins Removed",
      title = paste("Effect of Jamo Unicode Removal —", origin)
    )

  print(p)
})
```




